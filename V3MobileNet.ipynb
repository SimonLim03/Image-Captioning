{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Molly Dignan (24929263) - MobileNetV3 & LSTM Model"
      ],
      "metadata": {
        "id": "VyRjausrNFgK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URzP3a8hk5Sv",
        "outputId": "57cbb7ca-b4cd-4570-ecc8-16809cee16a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.0+cu121)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.14)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.40)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.7)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "!pip install torch torchvision nltk tqdm kaggle\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import nltk\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision import models\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Download NLTK punkt tokenizer models\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d 'adityajn105/flickr8k'\n",
        "import zipfile\n",
        "with zipfile.ZipFile('flickr8k.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('flickr8k')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FN1GTSFlIIQ",
        "outputId": "cdffc8cc-ccf0-4f40-da84-91a5266550f5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/adityajn105/flickr8k\n",
            "License(s): CC0-1.0\n",
            "flickr8k.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3VnZTPflKHR",
        "outputId": "54c2c36b-a955-401c-c82a-7efc1ca3434c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "captions_file = 'flickr8k/captions.txt'\n",
        "\n",
        "# Load and inspect captions file\n",
        "captions = pd.read_csv(captions_file, delimiter=',', header=None, names=['image', 'caption'])\n",
        "print(captions.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNaHJg_rlMDx",
        "outputId": "80f072f1-48fa-48f2-b344-d2ad72f903d8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                       image  \\\n",
            "0                      image   \n",
            "1  1000268201_693b08cb0e.jpg   \n",
            "2  1000268201_693b08cb0e.jpg   \n",
            "3  1000268201_693b08cb0e.jpg   \n",
            "4  1000268201_693b08cb0e.jpg   \n",
            "\n",
            "                                             caption  \n",
            "0                                            caption  \n",
            "1  A child in a pink dress is climbing up a set o...  \n",
            "2              A girl going into a wooden building .  \n",
            "3   A little girl climbing into a wooden playhouse .  \n",
            "4  A little girl climbing the stairs to her playh...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocabulary:\n",
        "    def __init__(self, freq_threshold):\n",
        "        self.freq_threshold = freq_threshold\n",
        "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
        "        self.stoi = {v: k for k, v in self.itos.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.itos)\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenizer(text):\n",
        "        return nltk.tokenize.word_tokenize(text.lower())\n",
        "\n",
        "    def build_vocabulary(self, sentence_list):\n",
        "        frequencies = Counter()\n",
        "        idx = 4\n",
        "\n",
        "        for sentence in sentence_list:\n",
        "            for word in self.tokenizer(sentence):\n",
        "                frequencies[word] += 1\n",
        "                if frequencies[word] == self.freq_threshold:\n",
        "                    self.stoi[word] = idx\n",
        "                    self.itos[idx] = word\n",
        "                    idx += 1\n",
        "\n",
        "    def numericalize(self, text):\n",
        "        tokenized_text = self.tokenizer(text)\n",
        "        return [self.stoi.get(token, self.stoi[\"<UNK>\"]) for token in tokenized_text]\n",
        "\n",
        "# Load and preprocess captions\n",
        "captions = pd.read_csv(captions_file, delimiter=',', header=None, names=['image', 'caption'])\n",
        "captions['caption'] = captions['caption'].fillna(\"\").astype(str)\n",
        "\n",
        "# Build the vocabulary\n",
        "vocab = Vocabulary(freq_threshold=5)\n",
        "vocab.build_vocabulary(captions['caption'].tolist())\n",
        "print(f\"Vocabulary size: {len(vocab)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tlZz9B_lSP1",
        "outputId": "b10d5c18-8101-49ab-dd57-3049dc634ad7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 3005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Flickr8kDataset(Dataset):\n",
        "    def __init__(self, img_dir, captions_df, transform=None, vocab=None, max_caption_length=20, train=True, test_split=0.2):\n",
        "        self.img_dir = img_dir\n",
        "        self.captions_df = captions_df.copy()\n",
        "        self.transform = transform\n",
        "        self.vocab = vocab\n",
        "        self.max_caption_length = max_caption_length\n",
        "        self.train = train\n",
        "        self.test_split = test_split\n",
        "\n",
        "        # Split dataset into training and testing sets\n",
        "        self.split_data()\n",
        "\n",
        "    def split_data(self):\n",
        "        if self.train:\n",
        "            train_size = int(len(self.captions_df) * (1 - self.test_split))\n",
        "            self.data = self.captions_df.iloc[:train_size].copy().reset_index(drop=True)\n",
        "        else:\n",
        "            test_size = int(len(self.captions_df) * self.test_split)\n",
        "            self.data = self.captions_df.iloc[-test_size:].copy().reset_index(drop=True)\n",
        "\n",
        "        # Pad captions to maximum length\n",
        "        self.pad_caption()\n",
        "\n",
        "    def pad_caption(self):\n",
        "        pad_idx = self.vocab.stoi['<PAD>']\n",
        "        for idx in range(len(self.data)):\n",
        "            numerical_caption = self.vocab.numericalize(self.data.at[idx, 'caption'])\n",
        "            while len(numerical_caption) < self.max_caption_length:\n",
        "                numerical_caption.append(pad_idx)\n",
        "            self.data.at[idx, 'caption'] = numerical_caption[:self.max_caption_length]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.img_dir, self.data.iloc[idx, 0])\n",
        "\n",
        "        try:\n",
        "            image = Image.open(img_name).convert(\"RGB\")\n",
        "        except FileNotFoundError:\n",
        "            print(f\"File not found: {img_name}\")\n",
        "            return None, None\n",
        "\n",
        "        caption = self.data.iloc[idx, 1]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, torch.tensor(caption)\n",
        "\n",
        "# Paths\n",
        "img_dir = 'flickr8k/Images'\n",
        "captions_file = 'flickr8k/captions.txt'\n",
        "\n",
        "# Load captions\n",
        "captions = pd.read_csv(captions_file, delimiter=',', header=None, names=['image', 'caption'])\n",
        "captions['caption'] = captions['caption'].fillna(\"\").astype(str)\n",
        "\n",
        "# Define vocabulary\n",
        "vocab = Vocabulary(freq_threshold=5)\n",
        "vocab.build_vocabulary(captions['caption'].values)\n",
        "\n",
        "# Data transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Define datasets for training and testing\n",
        "train_dataset = Flickr8kDataset(img_dir=img_dir, captions_df=captions, transform=transform, vocab=vocab, train=True)\n",
        "test_dataset = Flickr8kDataset(img_dir=img_dir, captions_df=captions, transform=transform, vocab=vocab, train=False)\n",
        "\n",
        "# Collate function to handle variable length sequences\n",
        "def collate_fn(batch):\n",
        "    batch = list(filter(lambda x: x[0] is not None, batch))\n",
        "    images, captions = zip(*batch)\n",
        "    images = torch.stack(images, 0)\n",
        "    captions = nn.utils.rnn.pad_sequence(captions, batch_first=True, padding_value=vocab.stoi[\"<PAD>\"])\n",
        "    return images, captions\n",
        "\n",
        "# Define data loaders for training and testing\n",
        "train_data_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, drop_last=True, collate_fn=collate_fn)\n",
        "test_data_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, drop_last=False, collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "id": "Qew0vok1lXSO"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MobileNetV3Extractor(nn.Module):\n",
        "    def __init__(self, embedding_dim=256):\n",
        "        super(MobileNetV3Extractor, self).__init__()\n",
        "        mobilenet_v3 = models.mobilenet_v3_large(pretrained=True)\n",
        "        self.features = mobilenet_v3.features\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc = nn.Linear(960, embedding_dim)  # Update the input size to match the actual size of the flattened tensor\n",
        "\n",
        "    def forward(self, images):\n",
        "        x = self.features(images)\n",
        "        x = self.pool(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "IfizyqIjlata"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, num_layers=1):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, features, captions):\n",
        "        embeddings = self.embedding(captions)\n",
        "        embeddings = torch.cat((features.unsqueeze(1), embeddings), dim=1)\n",
        "        lstm_out, _ = self.lstm(embeddings)\n",
        "        outputs = self.fc(lstm_out)\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "r2jjng9Xlb0l"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 256\n",
        "hidden_dim = 512\n",
        "vocab_size = len(vocab)\n",
        "num_epochs = 5\n",
        "learning_rate = 0.001\n",
        "print_every = 100\n",
        "\n",
        "# Model, Loss, Optimizer\n",
        "feature_extractor = MobileNetV3Extractor(embedding_dim=embedding_dim).to(device)\n",
        "decoder = Decoder(embedding_dim, hidden_dim, vocab_size).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(list(feature_extractor.parameters()) + list(decoder.parameters()), lr=learning_rate)\n",
        "\n",
        "total_steps = len(train_data_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, captions) in enumerate(train_data_loader):\n",
        "        images = images.to(device)\n",
        "        captions = captions.to(device)\n",
        "\n",
        "        # Pass image features through the decoder\n",
        "        features = feature_extractor(images)\n",
        "        outputs = decoder(features, captions[:, :-1])\n",
        "\n",
        "        # Calculate the loss\n",
        "        outputs = outputs[:, :captions.size(1) - 1, :]\n",
        "        loss = criterion(outputs.contiguous().view(-1, vocab_size), captions[:, 1:].contiguous().view(-1))\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print loss statistics\n",
        "        if i % print_every == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i}/{total_steps}], Loss: {loss.item():.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Z4h8ZcPlhWW",
        "outputId": "d9556e58-a89b-4ba9-ffca-7113c1a16229"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Large_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Step [0/1011], Loss: 8.0106\n",
            "Epoch [1/5], Step [100/1011], Loss: 2.5735\n",
            "Epoch [1/5], Step [200/1011], Loss: 2.6069\n",
            "Epoch [1/5], Step [300/1011], Loss: 2.7895\n",
            "Epoch [1/5], Step [400/1011], Loss: 2.6052\n",
            "Epoch [1/5], Step [500/1011], Loss: 2.6383\n",
            "Epoch [1/5], Step [600/1011], Loss: 2.5590\n",
            "Epoch [1/5], Step [700/1011], Loss: 2.4189\n",
            "File not found: flickr8k/Images/image\n",
            "Epoch [1/5], Step [800/1011], Loss: 2.3914\n",
            "Epoch [1/5], Step [900/1011], Loss: 2.4751\n",
            "Epoch [1/5], Step [1000/1011], Loss: 2.2339\n",
            "Epoch [2/5], Step [0/1011], Loss: 2.0035\n",
            "Epoch [2/5], Step [100/1011], Loss: 2.5033\n",
            "Epoch [2/5], Step [200/1011], Loss: 2.1039\n",
            "Epoch [2/5], Step [300/1011], Loss: 2.1454\n",
            "Epoch [2/5], Step [400/1011], Loss: 2.2350\n",
            "Epoch [2/5], Step [500/1011], Loss: 2.3710\n",
            "Epoch [2/5], Step [600/1011], Loss: 2.1364\n",
            "Epoch [2/5], Step [700/1011], Loss: 2.3653\n",
            "Epoch [2/5], Step [800/1011], Loss: 2.2136\n",
            "File not found: flickr8k/Images/image\n",
            "Epoch [2/5], Step [900/1011], Loss: 2.2941\n",
            "Epoch [2/5], Step [1000/1011], Loss: 2.5013\n",
            "Epoch [3/5], Step [0/1011], Loss: 1.8804\n",
            "Epoch [3/5], Step [100/1011], Loss: 2.2292\n",
            "Epoch [3/5], Step [200/1011], Loss: 2.3162\n",
            "Epoch [3/5], Step [300/1011], Loss: 2.1675\n",
            "Epoch [3/5], Step [400/1011], Loss: 2.3052\n",
            "Epoch [3/5], Step [500/1011], Loss: 2.0288\n",
            "Epoch [3/5], Step [600/1011], Loss: 2.0384\n",
            "Epoch [3/5], Step [700/1011], Loss: 2.0603\n",
            "Epoch [3/5], Step [800/1011], Loss: 2.3285\n",
            "Epoch [3/5], Step [900/1011], Loss: 2.0185\n",
            "Epoch [3/5], Step [1000/1011], Loss: 1.9817\n",
            "File not found: flickr8k/Images/image\n",
            "Epoch [4/5], Step [0/1011], Loss: 1.8077\n",
            "Epoch [4/5], Step [100/1011], Loss: 1.8586\n",
            "Epoch [4/5], Step [200/1011], Loss: 1.9368\n",
            "Epoch [4/5], Step [300/1011], Loss: 1.9210\n",
            "File not found: flickr8k/Images/image\n",
            "Epoch [4/5], Step [400/1011], Loss: 2.1506\n",
            "Epoch [4/5], Step [500/1011], Loss: 2.2342\n",
            "Epoch [4/5], Step [600/1011], Loss: 2.0522\n",
            "Epoch [4/5], Step [700/1011], Loss: 1.7915\n",
            "Epoch [4/5], Step [800/1011], Loss: 2.0166\n",
            "Epoch [4/5], Step [900/1011], Loss: 1.8724\n",
            "Epoch [4/5], Step [1000/1011], Loss: 1.9857\n",
            "Epoch [5/5], Step [0/1011], Loss: 1.5086\n",
            "Epoch [5/5], Step [100/1011], Loss: 1.7011\n",
            "Epoch [5/5], Step [200/1011], Loss: 1.7364\n",
            "Epoch [5/5], Step [300/1011], Loss: 2.0827\n",
            "Epoch [5/5], Step [400/1011], Loss: 1.9052\n",
            "Epoch [5/5], Step [500/1011], Loss: 1.7882\n",
            "Epoch [5/5], Step [600/1011], Loss: 1.8980\n",
            "Epoch [5/5], Step [700/1011], Loss: 1.9487\n",
            "Epoch [5/5], Step [800/1011], Loss: 1.8542\n",
            "Epoch [5/5], Step [900/1011], Loss: 1.6149\n",
            "File not found: flickr8k/Images/image\n",
            "Epoch [5/5], Step [1000/1011], Loss: 1.6771\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from tqdm import tqdm\n",
        "\n",
        "def evaluate_model(feature_extractor, decoder, test_data_loader, vocab, device):\n",
        "    # Set the model to evaluation mode\n",
        "    feature_extractor.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    references = []\n",
        "    hypotheses = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, captions in tqdm(test_data_loader):\n",
        "            images = images.to(device)\n",
        "            captions = captions.to(device)\n",
        "\n",
        "            # Pass image features through the decoder\n",
        "            features = feature_extractor(images)\n",
        "            outputs = decoder(features, captions[:, :-1])\n",
        "\n",
        "            # Get predicted captions\n",
        "            _, predicted = torch.max(outputs, dim=2)\n",
        "            predicted = predicted.tolist()\n",
        "            captions = captions[:, 1:].tolist()\n",
        "\n",
        "            # Convert numericalized captions to words\n",
        "            for p, c in zip(predicted, captions):\n",
        "                ref_sentence = [vocab.itos[word] for word in c if word != vocab.stoi['<PAD>']]\n",
        "                hyp_sentence = [vocab.itos[word] for word in p if word != vocab.stoi['<PAD>']]\n",
        "                references.append([ref_sentence])  # Note the double brackets\n",
        "                hypotheses.append(hyp_sentence)\n",
        "\n",
        "    # Calculate BLEU scores\n",
        "    bleu1 = corpus_bleu(references, hypotheses, weights=(1, 0, 0, 0))\n",
        "    bleu2 = corpus_bleu(references, hypotheses, weights=(0.5, 0.5, 0, 0))\n",
        "    bleu3 = corpus_bleu(references, hypotheses, weights=(0.33, 0.33, 0.33, 0))\n",
        "    bleu4 = corpus_bleu(references, hypotheses)\n",
        "\n",
        "    print(f'BLEU-1: {bleu1:.4f}')\n",
        "    print(f'BLEU-2: {bleu2:.4f}')\n",
        "    print(f'BLEU-3: {bleu3:.4f}')\n",
        "    print(f'BLEU-4: {bleu4:.4f}')\n",
        "\n",
        "# Run Evaluation on the Test Set\n",
        "evaluate_model(feature_extractor, decoder, test_data_loader, vocab, device)"
      ],
      "metadata": {
        "id": "tu_QcBsVmxgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference\n",
        "def generate_caption(image_path):\n",
        "    # Load and preprocess the image\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    feature_extractor.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    # Extract features\n",
        "    with torch.no_grad():\n",
        "        features = feature_extractor(image_tensor)\n",
        "\n",
        "    # Initialize the caption with the start token\n",
        "    caption = [vocab.stoi['<SOS>']]\n",
        "\n",
        "    while True:\n",
        "        # Convert caption to tensor\n",
        "        caption_tensor = torch.LongTensor(caption).unsqueeze(0).to(device)\n",
        "\n",
        "        # Pass features and current caption through the decoder\n",
        "        with torch.no_grad():\n",
        "            outputs = decoder(features, caption_tensor)\n",
        "\n",
        "        # Get the predicted word\n",
        "        predicted_word = outputs.argmax(2)[:,-1].item()\n",
        "\n",
        "        # Add the predicted word to the caption\n",
        "        caption.append(predicted_word)\n",
        "\n",
        "        # If the end token is predicted, stop generating\n",
        "        if predicted_word == vocab.stoi['<EOS>']:\n",
        "            break\n",
        "\n",
        "        # Stop if caption exceeds max length\n",
        "        if len(caption) >= 20:\n",
        "            break\n",
        "\n",
        "    # Convert the numericalized caption back to words\n",
        "    generated_caption = [vocab.itos[idx] for idx in caption]\n",
        "\n",
        "    # Return the generated caption\n",
        "    return ' '.join(generated_caption[1:-1])  # Exclude the start and end tokens\n",
        "\n",
        "# Example usage of generate_caption function\n",
        "image_path = 'flickr8k/Images/1000268201_693b08cb0e.jpg'\n",
        "caption = generate_caption(image_path)\n",
        "print(\"Generated Caption:\", caption)"
      ],
      "metadata": {
        "id": "gTuxTHEJm1yV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}